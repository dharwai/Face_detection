{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbb6b4d",
   "metadata": {},
   "source": [
    "# face expresion using model. it workng in fine laptop and pc both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ae8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.42.217:9090/ (Press CTRL+C to quit)\n",
      "192.168.42.217 - - [27/Mar/2023 13:20:25] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 434ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.42.217 - - [27/Mar/2023 13:20:35] \"POST /predict HTTP/1.1\" 302 -\n",
      "192.168.42.217 - - [27/Mar/2023 13:20:35] \"GET /result/Neutral HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from flask import Flask, request, jsonify, render_template, redirect, url_for\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tempfile\n",
    "\n",
    "#face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+ 'haarcascade_frontalface_default.xml')\n",
    "# Load the saved model\n",
    "model = load_model('model.h5')\n",
    "classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set the upload folder\n",
    "# Configure the temporary upload folder\n",
    "app.config['UPLOAD_FOLDER'] = 'C:/Users/RAJESH KUMAR/Desktop/working/working2/temporyry'\n",
    "\n",
    "\n",
    "# Define a function to preprocess the image\n",
    "def preprocess_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    image = image.convert('L')\n",
    "    # Resize the image to 48x48 pixels\n",
    "    image = image.resize((48, 48))\n",
    "    # Convert the image to a NumPy array\n",
    "    image = img_to_array(image)\n",
    "    # Reshape the array to (1, 48, 48, 1)\n",
    "    image = np.reshape(image, (1, 48, 48, 1))\n",
    "    # Normalize the image pixels to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    # Return the preprocessed image\n",
    "    return image\n",
    "\n",
    "# Define the route for the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Render the HTML template\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Define the route for the prediction API\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get the video file from the request\n",
    "    file = request.files['video']\n",
    "    \n",
    "    # Save the file to a temporary folder\n",
    "    file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
    "    file.save(file_path)\n",
    "    \n",
    "    # Read the video file as an OpenCV VideoCapture object\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    \n",
    "    # Create a window to display the video frames\n",
    "    cv2.namedWindow('Video', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Video', 640, 480)\n",
    "    \n",
    "    # Loop through the frames in the video\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        # If there are no more frames, break out of the loop\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the frame to a PIL image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Preprocess the image\n",
    "        image = preprocess_image(image)\n",
    "        \n",
    "        # Make a prediction using the model\n",
    "        prediction = model.predict(image)\n",
    "        \n",
    "        # Get the index of the predicted class\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        \n",
    "        # Get the predicted emotion label\n",
    "        emotion = classes[predicted_class]\n",
    "        \n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Draw the predicted emotion label on the frame\n",
    "        cv2.putText(frame, emotion, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "        # Wait for a key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the video object and close the window\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Redirect to the result page\n",
    "    return redirect(url_for('result', emotion=emotion))\n",
    "\n",
    "# Define the route for the result page\n",
    "@app.route('/result/<emotion>')\n",
    "def result(emotion):\n",
    "    # Render the result page\n",
    "    return render_template('result.html', emotion=emotion)\n",
    "\n",
    "\n",
    "            # Start the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=9090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9acb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e276747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932ca42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12f6512",
   "metadata": {},
   "source": [
    "# face expredion from video using deep face laptop working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.42.217:9090/ (Press CTRL+C to quit)\n",
      "192.168.42.217 - - [27/Mar/2023 13:12:33] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 223ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "sad\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "fear\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "angry\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.42.217 - - [27/Mar/2023 13:12:50] \"POST /predict HTTP/1.1\" 302 -\n",
      "192.168.42.217 - - [27/Mar/2023 13:12:50] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n"
     ]
    }
   ],
   "source": [
    "#Deepface way\n",
    "\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from flask import Flask, request, jsonify, render_template, redirect, url_for\n",
    "from deepface import DeepFace\n",
    "\n",
    "#face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+ 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# Configure the temporary upload folder\n",
    "app.config['UPLOAD_FOLDER'] = 'C:/Users/RAJESH KUMAR/Desktop/working/working2/temporyry'\n",
    "\n",
    "\n",
    "# Define the route for the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Render the HTML template\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Define the route for the prediction API\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get the video file from the request\n",
    "    file = request.files['video']\n",
    "    # Save the file to a temporary location on the server\n",
    "    file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
    "    file.save(file_path)\n",
    "    # Read the video file as an OpenCV VideoCapture object\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    # Loop through the frames in the video\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = video.read()\n",
    "        # If there are no more frames, break out of the loop\n",
    "        if not ret:\n",
    "            break\n",
    "        # Make a prediction using DeepFace library\n",
    "        prediction = DeepFace.analyze(frame, enforce_detection=False, actions=['emotion'])\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Get the predicted emotion label\n",
    "        predicted_emotion = prediction['dominant_emotion']\n",
    "        print(predicted_emotion)\n",
    "        # Display the predicted emotion on the frame\n",
    "        cv2.putText(frame,\n",
    "            prediction['dominant_emotion'],\n",
    "            (50,50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,3,\n",
    "            (255, 0, 0),\n",
    "            2,cv2.LINE_4);\n",
    "        #cv2.putText(frame, predicted_emotion, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        # Display the frame\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "    # Destroy all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "    # Redirect to the home page\n",
    "    return redirect(url_for('home'))\n",
    "            # Start the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=9090)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989df27",
   "metadata": {},
   "source": [
    "# if you are using deepfaec updated like 0.78 or higher the u can use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepface way\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from flask import Flask, request, jsonify, render_template, redirect, url_for\n",
    "from deepface import DeepFace\n",
    "\n",
    "#face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades+ 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure the temporary upload folder\n",
    "app.config['UPLOAD_FOLDER'] = 'C:/Users/Rajesh Kumar/Desktop/abcd/video flaskapp/temprory'\n",
    "\n",
    "\n",
    "# Define the route for the home page\n",
    "@app.route('/')\n",
    "def home():\n",
    "    # Render the HTML template\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Define the route for the prediction API\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get the video file from the request\n",
    "    file = request.files['video']\n",
    "    # Save the file to a temporary location on the server\n",
    "    file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)\n",
    "    file.save(file_path)\n",
    "    # Read the video file as an OpenCV VideoCapture object\n",
    "    video = cv2.VideoCapture(file_path)\n",
    "    # Loop through the frames in the video\n",
    "    while True:\n",
    "        # Read the next frame from the video\n",
    "        ret, frame = video.read()\n",
    "        # If there are no more frames, break out of the loop\n",
    "        if not ret:\n",
    "            break\n",
    "        # Make a prediction using DeepFace library\n",
    "        prediction = DeepFace.analyze(frame, enforce_detection=False, actions=['emotion'])\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Get the predicted emotion label\n",
    "        predicted_emotion = prediction[0]['dominant_emotion']\n",
    "        print(predicted_emotion)\n",
    "        # Display the predicted emotion on the frame\n",
    "        cv2.putText(frame,\n",
    "            prediction[0]['dominant_emotion'],\n",
    "            (50,50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,3,\n",
    "            (255, 0, 0),\n",
    "            2,cv2.LINE_4);\n",
    "        #cv2.putText(frame, predicted_emotion, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        # Display the frame\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "    # Destroy all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "    # Redirect to the home page\n",
    "    return redirect(url_for('home'))\n",
    "            # Start the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=9090)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
